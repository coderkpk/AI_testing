{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1cCjsiav_n_ML-KVWYEwThCA29FW3Mgd9",
      "authorship_tag": "ABX9TyNMVDXx4GJ/O5KjPO7Oo5pJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coderkpk/AI_testing/blob/main/FDHN(TC%2BTC%2BCB).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpG5uunG3ELx",
        "outputId": "7f1b6257-08a7-4315-a1e8-c8d6eea16fbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version : 2.6.0+cu124\n",
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-fd491cfeab4f>:109: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  train_label = torch.nn.functional.one_hot(torch.tensor(train_data['label'].replace(label_convert)), num_classes=6).type(torch.float64)\n",
            "<ipython-input-3-fd491cfeab4f>:117: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  train_dataset = LiarDataset(train_data, train_text, train_label, torch.tensor(train_data['label'].replace(label_convert)),\n",
            "<ipython-input-3-fd491cfeab4f>:126: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  val_label = torch.nn.functional.one_hot(torch.tensor(val_data['label'].replace(label_convert)), num_classes=6).type(torch.float64)\n",
            "<ipython-input-3-fd491cfeab4f>:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  val_dataset = LiarDataset(val_data, val_text, val_label, torch.tensor(val_data['label'].replace(label_convert)),\n",
            "<ipython-input-3-fd491cfeab4f>:143: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  test_label = torch.nn.functional.one_hot(torch.tensor(test_data['label'].replace(label_convert)), num_classes=6).type(torch.float64)\n",
            "<ipython-input-3-fd491cfeab4f>:151: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  test_dataset = LiarDataset(test_data, test_text, test_label, torch.tensor(test_data['label'].replace(label_convert)),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Best Result Updated at Epoch 1, Val Loss: 0.3855 *****\n",
            "Epoch [1/20], Time: 8.38s, Train Loss: 0.4326, Train Acc: 0.2657, Train F1 Macro: 0.2384, Train F1 Micro: 0.2657, Val Loss: 0.3855, Val Acc: 0.3941, Val F1 Macro: 0.3186, Val F1 Micro: 0.3941\n",
            "***** Best Result Updated at Epoch 2, Val Loss: 0.3586 *****\n",
            "Epoch [2/20], Time: 8.17s, Train Loss: 0.3746, Train Acc: 0.4234, Train F1 Macro: 0.3317, Train F1 Micro: 0.3446, Val Loss: 0.3586, Val Acc: 0.4704, Val F1 Macro: 0.4089, Val F1 Micro: 0.4322\n",
            "***** Best Result Updated at Epoch 3, Val Loss: 0.3488 *****\n",
            "Epoch [3/20], Time: 8.41s, Train Loss: 0.3431, Train Acc: 0.5015, Train F1 Macro: 0.3894, Train F1 Micro: 0.3969, Val Loss: 0.3488, Val Acc: 0.4618, Val F1 Macro: 0.4256, Val F1 Micro: 0.4421\n",
            "Epoch [4/20], Time: 8.42s, Train Loss: 0.3156, Train Acc: 0.5465, Train F1 Macro: 0.4293, Train F1 Micro: 0.4343, Val Loss: 0.3519, Val Acc: 0.4579, Val F1 Macro: 0.4345, Val F1 Micro: 0.4461\n",
            "***** Best Result Updated at Epoch 5, Val Loss: 0.3484 *****\n",
            "Epoch [5/20], Time: 8.41s, Train Loss: 0.2899, Train Acc: 0.5983, Train F1 Macro: 0.4635, Train F1 Micro: 0.4671, Val Loss: 0.3484, Val Acc: 0.4587, Val F1 Macro: 0.4383, Val F1 Micro: 0.4486\n",
            "Epoch [6/20], Time: 8.44s, Train Loss: 0.2593, Train Acc: 0.6611, Train F1 Macro: 0.4964, Train F1 Micro: 0.4994, Val Loss: 0.3650, Val Acc: 0.4540, Val F1 Macro: 0.4403, Val F1 Micro: 0.4495\n",
            "Epoch [7/20], Time: 8.97s, Train Loss: 0.2271, Train Acc: 0.7256, Train F1 Macro: 0.5293, Train F1 Micro: 0.5317, Val Loss: 0.3853, Val Acc: 0.4400, Val F1 Macro: 0.4410, Val F1 Micro: 0.4482\n",
            "Epoch [8/20], Time: 8.26s, Train Loss: 0.1946, Train Acc: 0.7846, Train F1 Macro: 0.5615, Train F1 Micro: 0.5633, Val Loss: 0.4045, Val Acc: 0.4486, Val F1 Macro: 0.4422, Val F1 Micro: 0.4482\n",
            "Epoch [9/20], Time: 8.51s, Train Loss: 0.1613, Train Acc: 0.8344, Train F1 Macro: 0.5916, Train F1 Micro: 0.5935, Val Loss: 0.4439, Val Acc: 0.4346, Val F1 Macro: 0.4409, Val F1 Micro: 0.4467\n",
            "Epoch [10/20], Time: 8.38s, Train Loss: 0.1415, Train Acc: 0.8683, Train F1 Macro: 0.6193, Train F1 Micro: 0.6209, Val Loss: 0.4656, Val Acc: 0.4268, Val F1 Macro: 0.4398, Val F1 Micro: 0.4447\n",
            "Epoch [11/20], Time: 8.30s, Train Loss: 0.1192, Train Acc: 0.8975, Train F1 Macro: 0.6447, Train F1 Micro: 0.6461, Val Loss: 0.4968, Val Acc: 0.4408, Val F1 Macro: 0.4398, Val F1 Micro: 0.4444\n",
            "Epoch [12/20], Time: 8.43s, Train Loss: 0.0955, Train Acc: 0.9305, Train F1 Macro: 0.6686, Train F1 Micro: 0.6698, Val Loss: 0.5364, Val Acc: 0.4408, Val F1 Macro: 0.4397, Val F1 Micro: 0.4441\n",
            "Epoch [13/20], Time: 8.46s, Train Loss: 0.0869, Train Acc: 0.9415, Train F1 Macro: 0.6897, Train F1 Micro: 0.6907, Val Loss: 0.5864, Val Acc: 0.4424, Val F1 Macro: 0.4397, Val F1 Micro: 0.4439\n",
            "Epoch [14/20], Time: 8.42s, Train Loss: 0.0748, Train Acc: 0.9518, Train F1 Macro: 0.7086, Train F1 Micro: 0.7093, Val Loss: 0.6081, Val Acc: 0.4393, Val F1 Macro: 0.4394, Val F1 Micro: 0.4436\n",
            "Epoch [15/20], Time: 8.51s, Train Loss: 0.0671, Train Acc: 0.9597, Train F1 Macro: 0.7254, Train F1 Micro: 0.7260, Val Loss: 0.6351, Val Acc: 0.4393, Val F1 Macro: 0.4391, Val F1 Micro: 0.4433\n",
            "Epoch [16/20], Time: 8.40s, Train Loss: 0.0576, Train Acc: 0.9683, Train F1 Macro: 0.7407, Train F1 Micro: 0.7412, Val Loss: 0.6814, Val Acc: 0.4268, Val F1 Macro: 0.4381, Val F1 Micro: 0.4423\n",
            "Epoch [17/20], Time: 8.47s, Train Loss: 0.0559, Train Acc: 0.9688, Train F1 Macro: 0.7542, Train F1 Micro: 0.7545, Val Loss: 0.7051, Val Acc: 0.4260, Val F1 Macro: 0.4374, Val F1 Micro: 0.4413\n",
            "Epoch [18/20], Time: 8.52s, Train Loss: 0.0502, Train Acc: 0.9743, Train F1 Macro: 0.7665, Train F1 Micro: 0.7668, Val Loss: 0.7312, Val Acc: 0.4283, Val F1 Macro: 0.4366, Val F1 Micro: 0.4406\n",
            "Epoch [19/20], Time: 8.33s, Train Loss: 0.0447, Train Acc: 0.9795, Train F1 Macro: 0.7778, Train F1 Micro: 0.7780, Val Loss: 0.7710, Val Acc: 0.4252, Val F1 Macro: 0.4358, Val F1 Micro: 0.4398\n",
            "Epoch [20/20], Time: 8.87s, Train Loss: 0.0431, Train Acc: 0.9802, Train F1 Macro: 0.7880, Train F1 Micro: 0.7881, Val Loss: 0.8331, Val Acc: 0.4245, Val F1 Macro: 0.4351, Val F1 Micro: 0.4390\n",
            "Total Training Time: 169.06s\n",
            "Test Loss: 0.3576, Test Acc: 0.4388, Test F1 Macro: 0.4425, Test F1 Micro: 0.4388\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Fixing the randomness of CUDA.\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"PyTorch Version : {}\".format(torch.__version__))\n",
        "print(DEVICE)\n",
        "\n",
        "\n",
        "worksapce = '/content/drive/MyDrive/liar/'\n",
        "model_save = 'TC+TC+CB.pt'\n",
        "model_name = 'TC+TC+CB'\n",
        "num_epochs = 20\n",
        "batch_size = 32\n",
        "learning_rate = 1e-3\n",
        "num_classes = 6\n",
        "padding_idx = 0\n",
        "metadata_each_dim = 10\n",
        "\n",
        "\n",
        "col = ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context']\n",
        "\n",
        "label_map = {0: 'pants-fire', 1: 'false', 2: 'barely-true', 3: 'half-true', 4: 'mostly-true', 5: 'true'}\n",
        "label_convert = {'pants-fire': 0, 'false': 1, 'barely-true': 2, 'half-true': 3, 'mostly-true': 4, 'true':5}\n",
        "\n",
        "train_data = pd.read_csv(worksapce + 'train.tsv', sep = '\\t', names = col)\n",
        "test_data = pd.read_csv(worksapce + 'test.tsv', sep = '\\t', names = col)\n",
        "val_data = pd.read_csv(worksapce + 'valid.tsv', sep = '\\t', names = col)\n",
        "\n",
        "# Replace NaN values with 'NaN'\n",
        "train_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']] = train_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']].fillna(0)\n",
        "train_data.fillna('NaN', inplace=True)\n",
        "\n",
        "test_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']] = test_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']].fillna(0)\n",
        "test_data.fillna('NaN', inplace=True)\n",
        "\n",
        "val_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']] = val_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']].fillna(0)\n",
        "val_data.fillna('NaN', inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def textProcess(input_text, max_length = -1):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    if max_length == -1:\n",
        "        tokens = tokenizer(input_text, truncation=True, padding=True)\n",
        "    else:\n",
        "        tokens = tokens = tokenizer(input_text, truncation=True, padding='max_length', max_length=max_length)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "\n",
        "# Define a custom dataset for loading the data\n",
        "class LiarDataset(data.Dataset):\n",
        "    def __init__(self, data_df, statement, label_onehot, label, subject, speaker, job_title, state_info,\n",
        "                     party_affiliation, barely_true_counts, false_counts, half_true_counts, mostly_true_counts,\n",
        "                    pants_on_fire_counts, context):\n",
        "        self.data_df = data_df\n",
        "        self.statement = statement\n",
        "        self.label_onehot = label_onehot\n",
        "        self.label = label\n",
        "        self.metadata_text = torch.cat((subject.int(), speaker.int(), job_title.int(), state_info.int(), party_affiliation.int(),\n",
        "                                   context.int()), dim=-1)\n",
        "        self.metadata_number = torch.cat((torch.tensor(barely_true_counts, dtype=torch.float).unsqueeze(1), torch.tensor(false_counts, dtype=torch.float).unsqueeze(1),\n",
        "                                   torch.tensor(half_true_counts, dtype=torch.float).unsqueeze(1), torch.tensor(mostly_true_counts, dtype=torch.float).unsqueeze(1),\n",
        "                                   torch.tensor(pants_on_fire_counts, dtype=torch.float).unsqueeze(1)), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        statement = self.statement[idx]\n",
        "        label_onehot = self.label_onehot[idx]\n",
        "        label = self.label[idx]\n",
        "        metadata_text = self.metadata_text[idx]\n",
        "        metadata_number = self.metadata_number[idx]\n",
        "        return statement, label_onehot, label, metadata_text, metadata_number\n",
        "\n",
        "\n",
        "# Define the data loaders for training and validation\n",
        "train_text = torch.tensor(textProcess(train_data['statement'].tolist())['input_ids'])\n",
        "train_label = torch.nn.functional.one_hot(torch.tensor(train_data['label'].replace(label_convert)), num_classes=6).type(torch.float64)\n",
        "train_subject = torch.tensor(textProcess(train_data['subject'].tolist(), metadata_each_dim)['input_ids'])\n",
        "train_speaker = torch.tensor(textProcess(train_data['speaker'].tolist(), metadata_each_dim)['input_ids'])\n",
        "train_job_title = torch.tensor(textProcess(train_data['job_title'].tolist(), metadata_each_dim)['input_ids'])\n",
        "train_state_info = torch.tensor(textProcess(train_data['state_info'].tolist(), metadata_each_dim)['input_ids'])\n",
        "train_party_affiliation = torch.tensor(textProcess(train_data['party_affiliation'].tolist(), metadata_each_dim)['input_ids'])\n",
        "train_context = torch.tensor(textProcess(train_data['context'].tolist(), metadata_each_dim)['input_ids'])\n",
        "\n",
        "train_dataset = LiarDataset(train_data, train_text, train_label, torch.tensor(train_data['label'].replace(label_convert)),\n",
        "                            train_subject, train_speaker, train_job_title,\n",
        "                            train_state_info, train_party_affiliation,\n",
        "                            train_data['barely_true_counts'].tolist(), train_data['false_counts'].tolist(),\n",
        "                            train_data['half_true_counts'].tolist(), train_data['mostly_true_counts'].tolist(),\n",
        "                            train_data['pants_on_fire_counts'].tolist(), train_context)\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_text = torch.tensor(textProcess(val_data['statement'].tolist())['input_ids'])\n",
        "val_label = torch.nn.functional.one_hot(torch.tensor(val_data['label'].replace(label_convert)), num_classes=6).type(torch.float64)\n",
        "val_subject = torch.tensor(textProcess(val_data['subject'].tolist(), metadata_each_dim)['input_ids'])\n",
        "val_speaker = torch.tensor(textProcess(val_data['speaker'].tolist(), metadata_each_dim)['input_ids'])\n",
        "val_job_title = torch.tensor(textProcess(val_data['job_title'].tolist(), metadata_each_dim)['input_ids'])\n",
        "val_state_info = torch.tensor(textProcess(val_data['state_info'].tolist(), metadata_each_dim)['input_ids'])\n",
        "val_party_affiliation = torch.tensor(textProcess(val_data['party_affiliation'].tolist(), metadata_each_dim)['input_ids'])\n",
        "val_context = torch.tensor(textProcess(val_data['context'].tolist(), metadata_each_dim)['input_ids'])\n",
        "\n",
        "val_dataset = LiarDataset(val_data, val_text, val_label, torch.tensor(val_data['label'].replace(label_convert)),\n",
        "                          val_subject, val_speaker, val_job_title,\n",
        "                          val_state_info, val_party_affiliation,\n",
        "                          val_data['barely_true_counts'].tolist(), val_data['false_counts'].tolist(),\n",
        "                          val_data['half_true_counts'].tolist(), val_data['mostly_true_counts'].tolist(),\n",
        "                          val_data['pants_on_fire_counts'].tolist(), val_context)\n",
        "val_loader = data.DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "test_text = torch.tensor(textProcess(test_data['statement'].tolist())['input_ids'])\n",
        "test_label = torch.nn.functional.one_hot(torch.tensor(test_data['label'].replace(label_convert)), num_classes=6).type(torch.float64)\n",
        "test_subject = torch.tensor(textProcess(test_data['subject'].tolist(), metadata_each_dim)['input_ids'])\n",
        "test_speaker = torch.tensor(textProcess(test_data['speaker'].tolist(), metadata_each_dim)['input_ids'])\n",
        "test_job_title = torch.tensor(textProcess(test_data['job_title'].tolist(), metadata_each_dim)['input_ids'])\n",
        "test_state_info = torch.tensor(textProcess(test_data['state_info'].tolist(), metadata_each_dim)['input_ids'])\n",
        "test_party_affiliation = torch.tensor(textProcess(test_data['party_affiliation'].tolist(), metadata_each_dim)['input_ids'])\n",
        "test_context = torch.tensor(textProcess(test_data['context'].tolist(), metadata_each_dim)['input_ids'])\n",
        "\n",
        "test_dataset = LiarDataset(test_data, test_text, test_label, torch.tensor(test_data['label'].replace(label_convert)),\n",
        "                          test_subject, test_speaker, test_job_title,\n",
        "                          test_state_info, test_party_affiliation,\n",
        "                          test_data['barely_true_counts'].tolist(), test_data['false_counts'].tolist(),\n",
        "                          test_data['half_true_counts'].tolist(), test_data['mostly_true_counts'].tolist(),\n",
        "                          test_data['pants_on_fire_counts'].tolist(), test_context)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv1d(in_channels = embedding_dim,\n",
        "                                              out_channels = n_filters,\n",
        "                                              kernel_size = fs)\n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        #text = [batch size, sent len]\n",
        "        embedded = self.embedding(text)\n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "        #embedded = [batch size, emb dim, sent len]\n",
        "\n",
        "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        #pooled_n = [batch size, n_filters]\n",
        "\n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "\n",
        "        return self.fc(cat)\n",
        "\n",
        "class CNNBiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
        "        self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=32, kernel_size=1)\n",
        "        self.rnn = nn.LSTM(32,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, metadata):\n",
        "        #metadata = [batch size, metadata dim]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(metadata))\n",
        "        #embedded = [batch size, metadata dim, emb dim]\n",
        "\n",
        "        embedded = torch.reshape(embedded, (metadata.size(0), 128, 1))\n",
        "\n",
        "        conved = F.relu(self.conv(embedded))\n",
        "        #conved = [batch size, n_filters, metadata dim - filter_sizes[n] + 1]\n",
        "\n",
        "        conved = torch.reshape(conved, (metadata.size(0), 32))\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(conved)\n",
        "        #outputs = [metadata dim - filter_sizes[n] + 1, batch size, hid dim * num directions]\n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "\n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        #and apply dropout\n",
        "        # hidden = self.dropout(torch.cat((hidden[-1,:], hidden[0,:]), dim = -1))\n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "\n",
        "        return self.fc(outputs)\n",
        "\n",
        "\n",
        "class LiarModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, padding_idx, input_dim, input_dim_metadata, hidden_dim, n_layers, bidirectional):\n",
        "        super().__init__()\n",
        "\n",
        "        self.textcnn = TextCNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, padding_idx)\n",
        "        self.textcnn2 = TextCNN(vocab_size, input_dim, n_filters, filter_sizes, output_dim, dropout, padding_idx)\n",
        "        self.cnn_bilstm = CNNBiLSTM(input_dim_metadata, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout)\n",
        "        self.fuse = nn.Linear(output_dim * 3, output_dim)\n",
        "\n",
        "    def forward(self, text, metadata_text, metadata_number):\n",
        "        #text = [batch size, sent len]\n",
        "        #metadata = [batch size, metadata dim]\n",
        "\n",
        "        text_output = self.textcnn(text)\n",
        "        metadata_output_text = self.textcnn2(metadata_text)\n",
        "        metadata_output_number = self.cnn_bilstm(metadata_number)\n",
        "\n",
        "        fused_output = self.fuse(torch.cat((text_output, metadata_output_text, metadata_output_number), dim=1))\n",
        "\n",
        "        return fused_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vocab_size = 30522\n",
        "embedding_dim = 128\n",
        "n_filters = 128\n",
        "filter_sizes = [3,4,5]\n",
        "output_dim = 6\n",
        "dropout = 0.5\n",
        "padding_idx = 0\n",
        "input_dim = 6 * metadata_each_dim\n",
        "input_dim_metadata = 5\n",
        "hidden_dim = 64\n",
        "n_layers = 1\n",
        "bidirectional = True\n",
        "\n",
        "model = LiarModel(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, padding_idx, input_dim, input_dim_metadata, hidden_dim, n_layers, bidirectional).to(DEVICE)\n",
        "\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "# Record the training process\n",
        "Train_acc = []\n",
        "Train_loss = []\n",
        "Train_macro_f1 = []\n",
        "Train_micro_f1 = []\n",
        "\n",
        "Val_acc = []\n",
        "Val_loss = []\n",
        "Val_macro_f1 = []\n",
        "Val_micro_f1 = []\n",
        "\n",
        "def train(num_epochs, model, train_loader, val_loader, optimizer, criterion, model_save):\n",
        "    epoch_trained = 0\n",
        "    train_label_all = []\n",
        "    train_predict_all = []\n",
        "    val_label_all = []\n",
        "    val_predict_all = []\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_trained += 1\n",
        "        epoch_start_time = time.time()\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_accuracy = 0.0\n",
        "        for statements, label_onehot, label, metadata_text, metadata_number in train_loader:\n",
        "            statements = statements.to(DEVICE)\n",
        "            label_onehot = label_onehot.to(DEVICE)\n",
        "            label = label.to(DEVICE)\n",
        "            metadata_text = metadata_text.to(DEVICE)\n",
        "            metadata_number = metadata_number.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(statements, metadata_text, metadata_number)\n",
        "            loss = criterion(outputs, label_onehot)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, train_predicted = torch.max(outputs, 1)\n",
        "            train_accuracy += sum(train_predicted == label)\n",
        "            train_predict_all += train_predicted.tolist()\n",
        "            train_label_all += label.tolist()\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy /= len(train_loader.dataset)\n",
        "        train_macro_f1 = f1_score(train_label_all, train_predict_all, average='macro')\n",
        "        train_micro_f1 = f1_score(train_label_all, train_predict_all, average='micro')\n",
        "\n",
        "        Train_acc.append(train_accuracy.tolist())\n",
        "        Train_loss.append(train_loss)\n",
        "        Train_macro_f1.append(train_macro_f1)\n",
        "        Train_micro_f1.append(train_micro_f1)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_accuracy = 0.0\n",
        "        with torch.no_grad():\n",
        "            for statements, label_onehot, label, metadata_text, metadata_number in val_loader:\n",
        "                statements = statements.to(DEVICE)\n",
        "                label_onehot = label_onehot.to(DEVICE)\n",
        "                label = label.to(DEVICE)\n",
        "                metadata_text = metadata_text.to(DEVICE)\n",
        "                metadata_number = metadata_number.to(DEVICE)\n",
        "\n",
        "                val_outputs = model(statements, metadata_text, metadata_number)\n",
        "                val_loss += criterion(val_outputs, label_onehot).item()\n",
        "                _, val_predicted = torch.max(val_outputs, 1)\n",
        "                val_accuracy += sum(val_predicted == label)\n",
        "                val_predict_all += val_predicted.tolist()\n",
        "                val_label_all += label.tolist()\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy /= len(val_loader.dataset)\n",
        "        val_macro_f1 = f1_score(val_label_all, val_predict_all, average='macro')\n",
        "        val_micro_f1 = f1_score(val_label_all, val_predict_all, average='micro')\n",
        "\n",
        "        Val_acc.append(val_accuracy.tolist())\n",
        "        Val_loss.append(val_loss)\n",
        "        Val_macro_f1.append(val_macro_f1)\n",
        "        Val_micro_f1.append(val_micro_f1)\n",
        "\n",
        "        if val_loss < best_valid_loss:\n",
        "            best_valid_loss = val_loss\n",
        "            torch.save(model.state_dict(), model_save)\n",
        "            print(f'***** Best Result Updated at Epoch {epoch_trained}, Val Loss: {val_loss:.4f} *****')\n",
        "\n",
        "        # Print the losses and accuracy\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Time: {epoch_time:.2f}s, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Train F1 Macro: {train_macro_f1:.4f}, Train F1 Micro: {train_micro_f1:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1 Macro: {val_macro_f1:.4f}, Val F1 Micro: {val_micro_f1:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    print(f'Total Training Time: {training_time:.2f}s')\n",
        "\n",
        "\n",
        "train(num_epochs, model, train_loader, val_loader, optimizer, criterion, model_save)\n",
        "\n",
        "\n",
        "# Evaluate the model on new data\n",
        "def test(model, test_loader, model_save):\n",
        "    model.load_state_dict(torch.load(model_save))\n",
        "    model.eval()\n",
        "\n",
        "    test_label_all = []\n",
        "    test_predict_all = []\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_accuracy = 0.0\n",
        "    with torch.no_grad():\n",
        "        for statements, label_onehot, label, metadata_text, metadata_number in test_loader:\n",
        "            statements = statements.to(DEVICE)\n",
        "            label_onehot = label_onehot.to(DEVICE)\n",
        "            label = label.to(DEVICE)\n",
        "            metadata_text = metadata_text.to(DEVICE)\n",
        "            metadata_number = metadata_number.to(DEVICE)\n",
        "\n",
        "            test_outputs = model(statements, metadata_text, metadata_number)\n",
        "            test_loss += criterion(test_outputs, label_onehot).item()\n",
        "            _, test_predicted = torch.max(test_outputs, 1)\n",
        "\n",
        "            test_accuracy += sum(test_predicted == label)\n",
        "            test_predict_all += test_predicted.tolist()\n",
        "            test_label_all += label.tolist()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy /= len(test_loader.dataset)\n",
        "    test_macro_f1 = f1_score(test_label_all, test_predict_all, average='macro')\n",
        "    test_micro_f1 = f1_score(test_label_all, test_predict_all, average='micro')\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}, Test F1 Macro: {test_macro_f1:.4f}, Test F1 Micro: {test_micro_f1:.4f}')\n",
        "\n",
        "\n",
        "test(model, test_loader, model_save)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ol1XExkz6T3-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}